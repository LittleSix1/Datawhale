{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【Task2 GBDT算法梳理】3天  \n",
    "【参考框架】欢迎有自己的框架  \n",
    "前向分布算法  \n",
    "负梯度拟合   \n",
    "损失函数  \n",
    "回归  \n",
    "二分类，多分类  \n",
    "正则化  \n",
    "优缺点  \n",
    "sklearn参数  \n",
    "应用场景  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 GBDT简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前提到了boosting 和 bagging集成算法，我们知道随机森林属于bagging,这次我们来具体看下boosting的代表算法GBDT(梯度提升树Gradient Boosting Decision Tree).  虽然GBDT和Adaboost 都属于boosting算法，但是他们之间存在很大不同，具体如下，我们主要围绕下面这几个概念展开讨论。\n",
    "\n",
    "Boosting（提升方法） = 加法模型 + 前向分步算法 + 损失函数  \n",
    "AdaBoost = Boosting + 损失函数是指数函数（基函数任意）  \n",
    "Boosting Tree（提升树） = Boosting + 基函数是决策树（损失函数任意）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost 是利用前一轮弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去，简单的说是Boosting框架+任意基学习器算法+指数损失函数。GBDT也是迭代，也使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同，简单的说Boosting框架+CART回归树模型+任意损失函数。  \n",
    "GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 前向分布算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向分步算法的求解思路就是把上述同时求多个参数拆解为每一步只求一个，更具体的是每一步只学习一个基函数及其系数，逐步逼近上述的目标函数，所以，前向分布算法每一步的优化目标为：\n",
    "每次仅仅只学习一个基函数及其系数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.负梯度拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升方法实际采用加法模型与前向分步算法，以决策树作为基函数的提升方法称为提升树。注意，这里的决策树为CART回归树，不是分类树。当问题是分类问题时，采用的决策树模型为分类回归树。  \n",
    "提升树用加法模型与前向分布算法实现学习的优化过程，当损失函数为平方损失函数和指数损失函数时，优化问题较简单。而对于一般的损失函数并不容易，这时候一般利用梯度下降来进行优化，而我们一般用负梯度拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类算法  \n",
    "对数损失函数  \n",
    "指数损失函数 \n",
    "\n",
    "回归算法  \n",
    "平方损失函数  \n",
    "绝对损失函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.正则化\n",
    "三种方法：  \n",
    "给没课树输出结果加一个步长η (学习率)  \n",
    "子采样比例  \n",
    "正则化剪枝  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.优缺点\n",
    "优点：  \n",
    "可以处理连续值和离散值  \n",
    "较小调参的情况下效果也比较好  \n",
    "对异常值有较好的鲁棒性  \n",
    "\n",
    "缺点：  \n",
    "弱学习器之间有较强依耐性，难以并行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.sklearn参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, \n",
    "                subsample=1.0, criterion=’friedman_mse’, min_samples_split=2,\n",
    "                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n",
    "                random_state=None, max_features=None, alpha=0.9, verbose=0, \n",
    "                max_leaf_nodes=None, warm_start=False, presort=’auto’, \n",
    "                validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "\n",
    "n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。  \n",
    "learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。  \n",
    "subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。  \n",
    "init：我们初始化的时候的弱学习器。若不设置，则使用默认的。  \n",
    "loss：损失函数，可选{‘ls’-平方损失函数，‘lad’绝对损失函数-,‘huber’-huber损失函数,‘quantile’-分位数损失函数}，默认’ls’。  \n",
    "alpha：当我们在使用Huber损失\"Huber\"和分位数损失\"quantile\"时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。  \n",
    "criterion：决策树节搜索最优分割点的准则，默认是\"friedman_mse\"，可选\"mse\"-均方误差与’mae\"-绝对误差。  \n",
    "max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。  \n",
    "max_depth：树的最大深度。  \n",
    "min_samples_split：内部节点再划分所需最小样本数。  \n",
    "min_samples_leaf：叶子节点最少样本数。  \n",
    "max_leaf_nodes：最大叶子节点数。  \n",
    "min_impurity_split：节点划分最小不纯度。  \n",
    "presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。  \n",
    "validationfraction：为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。  \n",
    "n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.应用场景\n",
    "几乎所有的回归问题  \n",
    "分类问题  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "主要参考了本次一起学习的同学的博客  \n",
    "https://blog.csdn.net/fat_cai_niao/article/details/89065666  \n",
    "https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
