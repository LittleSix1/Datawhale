{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Datawhale | 高级算法梳理第6期  \n",
    "\n",
    "【Task1(2天)】随机森林算法梳理\n",
    "1. 集成学习概念\n",
    "2. 个体学习器概念\n",
    "3. boosting bagging\n",
    "4. 结合策略(平均法，投票法，学习法)\n",
    "5. 随机森林思想\n",
    "6. 随机森林的推广\n",
    "7. 优缺点\n",
    "8. sklearn参数\n",
    "9. 应用场景"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 理论背景\n",
    "    在介绍集成学习之前先介绍几个概念   PAC Learnability(PAC 概率近似正确)  PAC 定义了学习算法的强弱      \n",
    "    弱学习算法---识别错误率小于1/2(即准确率仅比随机猜测略高的学习算法)，弱学习器\n",
    "    强学习算法---识别准确率很高并能在多项式时间内完成的学习算法，强学习器\n",
    "![image.png](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike72%2C5%2C5%2C72%2C24/sign=beb270a4dd1373f0e13267cdc566209e/023b5bb5c9ea15ce592208d6ba003af33b87b225.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 个体学习器\n",
    "    个体学习器通常是用一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法、SVM、线性回归、逻辑回归、朴素贝叶斯等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 集成学习概念\n",
    "       \n",
    "    在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。\n",
    "\n",
    "    根据个体学习器（弱分类器、基学习器）的集成种类，集成学习可以分为：\n",
    "    同质集成：只包含同种类型的个体学习器\n",
    "    异质集成：个体学习器由不同的学习算法生成。\n",
    "    \n",
    "    随着集成学习的发展，其广义的定义逐渐被学者们所接受，它是指对多个学习器集合采用学习的方式，而不对学习器性质加以区分。但是目前为止大家还是以同质的分类器的集成学习研究居多。个人认为，异质的集成方式可以被称作模型融合，集成学习的概念比较广，集成学习包含模型融合。也可以说把强分类器进行强强联合，使得融合后的模型效果更强，称为模型融合。\n",
    "    \n",
    "    集成学习的主要思路是先通过一定的规则生成多个体学习器，再采用某种集成策略进行组合，最后综合判断输出最终结果。\n",
    "![image.png](https://raw.githubusercontent.com/LittleSix1/img_hub/master/ensemble_learning/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.jpg)\n",
    "\n",
    "    从个体学习器之间是否存在依赖关系来看，集成学习分为：boosting（串行，代表方法Adaboost、GBDT、XGB）、bagging（并行，代表方法随机森林）\n",
    "    \n",
    "    从结合策略来看，目前主要的结合策略有：平均法、投票法、学习法（包括Stacking、Blending）\n",
    "    下面一一介绍"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. bagging&boosting \n",
    "- bagging算法  \n",
    "\n",
    "      bagging算法，它的个体学习器之间没有依赖关系，可以并行生成。如下图：\n",
    "![image.png](https://raw.githubusercontent.com/LittleSix1/img_hub/master/ensemble_learning/bagging.jpg)\n",
    "\n",
    "      1 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "    \n",
    "      2 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "    \n",
    "      3 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n",
    "    \n",
    "      随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- boosting  \n",
    "      bagging算法，它的个体学习器之间存在依赖关系，不可并行生成。如下图：\n",
    "![image.png](https://raw.githubusercontent.com/LittleSix1/img_hub/master/ensemble_learning/boosting.jpg)\n",
    "\n",
    "      Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。  \n",
    "    \n",
    "      Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 结合策略(平均法，投票法，学习法)\n",
    "\n",
    "    模型融合的结合策略： 个体学习器学习完后，需要将各个模型进行融合，常见的有三种\n",
    "    \n",
    "    平均法： 平均法有一般的平均和加权平均，这个好理解。对于平均法来说一般用于回归预测模型中，在Boosting系列融合模型中，一般采用的是加权平均融合。\n",
    "\n",
    "    投票法：有绝对多数投票（得票超过一半），相对多数投票（得票最多），加权投票。这个也好理解，一般用于分类模型。在bagging模型中使用。\n",
    "\n",
    "    学习法：一种更为强大的结合策略是使用”学习法”，即通过另一个学习器来进行结合，把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。 常见的有Stacking和Blending两种。\n",
    "    \n",
    "    下面着重讲一下学习法的结合策略，Stacking和Blending，比较常用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking\n",
    "\n",
    "        stacking的过程有一张图非常经典，如下：\n",
    "![image.png](https://raw.githubusercontent.com/LittleSix1/img_hub/master/ensemble_learning/stacking.jpg)\n",
    "\n",
    "        上半部分是用一个基础模型进行5折交叉验证。Taining Data中的Learn表示训练集，Predict表示验证集，Test Data为测试集；\n",
    "\n",
    "        每一次的交叉验证包含两个过程，\n",
    "\n",
    "        1. 基于training data训练模型；\n",
    "\n",
    "        2. 基于training data训练生成的模型对testing data进行预测。\n",
    "\n",
    "        在整个交叉验证的第一次完成之后我们将会得到关于当前验证集的预测值，记为a1。在这部分操作完成后，我们还要对数据集的整个testing set进行预测，这部分预测值将会作为下一层模型testing data的一部分，记为b1。因为我们进行的是5折交叉验证，所以以上提及的过程将会进行五次，最终生成数据a1,a2,a3,a4,a5，对testing set的预测数据b1,b2,b3,b4,b5。\n",
    "\n",
    "        我们可以发现a1,a2,a3,a4,a5其实就是对原来整个training set的预测值，将他们拼凑起来，记为A1。而对于b1,b2,b3,b4,b5这部分数据，我们将各部分相加取平均值，记为B1。\n",
    "\n",
    "        stacking中同一层通常包含多个模型，假设还有Model2，Model3，Model4，Model5，对于这四个模型，我们可以重复以上的步骤，在整个流程结束之后，我们可以得到新的A2,A3,A4,A5,B2,B3,B4,B5矩阵。之后，我们把A1,A2,A3,A4,A5并列合并得到一个的矩阵作为training data，B1,B2,B3,B4,B5并列合并得到的矩阵作为testing data。作为下一层模型的输入数据。\n",
    "\n",
    "        通俗的将，就是将上一个模型的输出作为下一个模型的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 随机森林思想\n",
    "\n",
    "    随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。 \n",
    "    \n",
    "    RF使用了CART决策树作为弱学习器。在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为nsub，然后在这些随机选择的nsub个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　\n",
    "    \n",
    "    如果nsub=n，则此时RF的CART决策树和普通的CART决策树没有区别。nsub越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说nsub越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的nsub的值。\n",
    "    \n",
    "    除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。\n",
    "        输入为样本集D={(x,y1),(x2,y2),...(xm,ym)}D={(x,y1),(x2,y2),...(xm,ym)}，弱分类器迭代次数T。\n",
    "        输出为最终的强分类器f(x)f(x)  \n",
    "        \n",
    "        1）对于t=1,2...,T:\n",
    "        a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集DtDt\n",
    "        b)用采样集DtDt训练第t个决策树模型Gt(x)Gt(x)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分\n",
    "\n",
    "        2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 随机森林的推广\n",
    "\n",
    "- extra trees\n",
    "\n",
    "        extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：\n",
    "\n",
    "        1） 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。\n",
    "\n",
    "        2） 在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。\n",
    "\n",
    "        从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好。\n",
    "        \n",
    "- Totally Random Trees Embedding\n",
    "\n",
    "        Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。\n",
    "\n",
    "        TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征xx划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,     0,0,1,0,0,     0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。\n",
    "\n",
    "        映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。\n",
    "\n",
    "- Isolation Forest\n",
    "\n",
    "        Isolation Forest（以下简称IForest）是一种异常点检测的方法。它也使用了类似于RF的方法来检测异常点。\n",
    "\n",
    "        对于在T个决策树的样本集，IForest也会对训练集进行随机采样,但是采样个数不需要和RF一样，对于RF，需要采样到采样集样本个数等于训练集个数。但是IForest不需要采样这么多，一般来说，采样个数要远远小于训练集个数？为什么呢？因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。\n",
    "\n",
    "        对于每一个决策树的建立， IForest采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。这点也和RF不同。\n",
    "\n",
    "        另外，IForest一般会选择一个比较小的最大决策树深度max_depth,原因同样本采集，用少量的异常点检测一般不需要这么大规模的决策树。\n",
    "\n",
    "        对于异常点的判断，则是将测试样本点xx拟合到T颗决策树。计算在每颗决策树上该样本的叶子节点的深度ht(x)。，从而可以计算出平均高度h(x)。此时我们用下面的公式计算样本点xx的异常概率:\n",
    "        s(x,m)=2exp（−h(x)/c(m)）\n",
    "        其中，m为样本个数。c(m)c(m)的表达式为：\n",
    "        c(m)=2ln(m−1)+ξ−2m−1m,ξ为欧拉常数\n",
    "        s(x,m)的取值范围是[0,1],取值越接近于1，则是异常点的概率也越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 优缺点\n",
    "    RF是一个可以高度并行化的算法，RF在大数据时候大有可为。 这里也对常规的随机森林算法的优缺点做一个总结。\n",
    "\n",
    "    RF的主要优点有：\n",
    "\n",
    "    1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。\n",
    "\n",
    "    2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。\n",
    "\n",
    "    3） 在训练后，可以给出各个特征对于输出的重要性\n",
    "\n",
    "    4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。\n",
    "\n",
    "    5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。\n",
    "\n",
    "    6） 对部分特征缺失不敏感。\n",
    "\n",
    "    RF的主要缺点有：\n",
    "\n",
    "    1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。\n",
    "\n",
    "    2) 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. sklearn参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table border=\"1\" class=\"dataframe\" >\n",
    "  <thead>\n",
    "    <tr style=\"text-align: center;\">\n",
    "      <th></th>\n",
    "      <th>参数</th>\n",
    "      <th>类型</th>\n",
    "      <th>默认值</th>\n",
    "      <th>作用</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>n_estimators</td>\n",
    "      <td>int</td>\n",
    "      <td>10</td>\n",
    "      <td>树的棵树</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>criterion</td>\n",
    "      <td>str</td>\n",
    "      <td>‘gini’</td>\n",
    "      <td>分裂算法</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>max_depth</td>\n",
    "      <td>int or None</td>\n",
    "      <td>None</td>\n",
    "      <td>决策树最大深度</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>min_samples_split</td>\n",
    "      <td>int or float</td>\n",
    "      <td>2</td>\n",
    "      <td>分裂时最小样本数</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>min_samples_leaf</td>\n",
    "      <td>int or float</td>\n",
    "      <td>1</td>\n",
    "      <td>叶节点最小样本数</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>min_weight_fraction_leaf</td>\n",
    "      <td>float</td>\n",
    "      <td>0</td>\n",
    "      <td>叶节点最小样本权重总值</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>max_features</td>\n",
    "      <td>int float str None</td>\n",
    "      <td>‘auto’</td>\n",
    "      <td>切分时最大的特征数量</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>max_leaf_nodes</td>\n",
    "      <td>int or None</td>\n",
    "      <td>None</td>\n",
    "      <td>最大叶节点个数</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>min_impurity_decrease</td>\n",
    "      <td>float</td>\n",
    "      <td>0</td>\n",
    "      <td>切分点不纯度最小减少程度,若节点不纯度小于该值,则被移除</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>bootstrap</td>\n",
    "      <td>boolean</td>\n",
    "      <td>1</td>\n",
    "      <td>是否bootstrap</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>oob_score</td>\n",
    "      <td>boolean</td>\n",
    "      <td>0</td>\n",
    "      <td>是否用未采样的数据来验证模型得分</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>n_jobs</td>\n",
    "      <td>int or None</td>\n",
    "      <td>None</td>\n",
    "      <td>并行数, -1为适用所有进程, 即与内核数相同, None为1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>random_state</td>\n",
    "      <td>int or None</td>\n",
    "      <td>None</td>\n",
    "      <td>随机种子</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>verbose</td>\n",
    "      <td>int</td>\n",
    "      <td>0</td>\n",
    "      <td>日志输出间隔</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>warm_start</td>\n",
    "      <td>boolean</td>\n",
    "      <td>0</td>\n",
    "      <td>是否热启动，对上一个模型采用追加的方式</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 应用场景\n",
    "    1. 很容易处理含有类别型、数值型等不同类型特征的数据.\n",
    "    2. 适用于处理维数较高的数据集.?  适合吗  每次调参时 速度都很慢\n",
    "    3. 由于随机性的存在，RF不需要对决策树进行剪枝操作，也可以拥有良好的泛化能力和抗过拟合能力."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "『我爱机器学习』集成学习（一）模型融合与Bagging  https://www.hrwhisper.me/machine-learning-model-ensemble-and-bagging  \n",
    "集成学习原理小结 https://www.cnblogs.com/pinard/p/6131423.html  \n",
    "集成学习-模型融合学习笔记（附Python代码） https://blog.csdn.net/u012735708/article/details/82349731  \n",
    "Bagging与随机森林算法原理小结 http://www.cnblogs.com/pinard/p/6156009.html  \n",
    "集成学习算法梳理——RF https://blog.csdn.net/JN_rainbow/article/details/88993591  \n",
    "基于scikit-learn的随机森林调参实战 https://blog.csdn.net/Kaiyuan_sjtu/article/details/80173417"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
